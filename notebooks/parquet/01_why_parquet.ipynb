{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938d274c",
   "metadata": {},
   "source": [
    "# Why Parquet?\n",
    "\n",
    "> **Level:** Beginner  \n",
    "> **Spec:** [Motivation](https://parquet.apache.org/docs/overview/motivation/)  \n",
    "> **PyArrow docs:** [Reading and Writing the Apache Parquet Format](https://arrow.apache.org/docs/python/parquet.html)\n",
    "\n",
    "**What you will learn:**\n",
    "\n",
    "1. Why columnar storage outperforms row-oriented formats for analytical queries\n",
    "2. How Parquet compares to CSV in file size and read latency\n",
    "3. How column projection avoids reading irrelevant data entirely\n",
    "4. Where Parquet fits in the broader data ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "57c92435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pa_csv\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444a404",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Build a sample dataset\n",
    "\n",
    "> **Spec:** [Overview](https://parquet.apache.org/docs/overview/)\n",
    "\n",
    "We create a table with 1 million rows and four columns of different types.\n",
    "This is large enough for the size and speed differences to be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 20,000,000\n",
      "Columns: 4\n",
      "Schema: id: int64\n",
      "value: double\n",
      "category: string\n",
      "flag: bool\n"
     ]
    }
   ],
   "source": [
    "N = 1_000_000\n",
    "categories = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\"]\n",
    "\n",
    "table = pa.table({\n",
    "    \"id\":       pa.array(range(N), type=pa.int64()),\n",
    "    \"value\":    pa.array([float(i) * 0.1 for i in range(N)], type=pa.float64()),\n",
    "    \"category\": pa.array([categories[i % len(categories)] for i in range(N)], type=pa.string()),\n",
    "    \"flag\":     pa.array([i % 2 == 0 for i in range(N)], type=pa.bool_()),\n",
    "})\n",
    "\n",
    "print(f\"Rows: {table.num_rows:,}\")\n",
    "print(f\"Columns: {table.num_columns}\")\n",
    "print(f\"Schema: {table.schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a7e86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Write as CSV and Parquet, compare file size\n",
    "\n",
    "> **Spec:** [Motivation: compression](https://parquet.apache.org/docs/overview/motivation/)\n",
    "\n",
    "Parquet applies per-column encoding and compression by default.  \n",
    "CSV stores every value as a plain text character, with no type awareness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c13703d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV size:     662.20 MB\n",
      "Parquet size: 66.85 MB\n",
      "Ratio:        Parquet is 9.9x smaller than CSV\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/tmp/sample.csv\"\n",
    "parquet_path = \"/tmp/sample.parquet\"\n",
    "\n",
    "os.remove(csv_path) if os.path.exists(csv_path) else None\n",
    "os.remove(parquet_path) if os.path.exists(parquet_path) else None\n",
    "\n",
    "# Write CSV\n",
    "csv_buf = io.BytesIO()\n",
    "pa_csv.write_csv(table, csv_buf)\n",
    "with open(csv_path, \"wb\") as f:\n",
    "    f.write(csv_buf.getvalue())\n",
    "\n",
    "# Write Parquet\n",
    "pq.write_table(table, parquet_path, compression='zstd', row_group_size=N / 50)\n",
    "\n",
    "csv_size = os.path.getsize(csv_path)\n",
    "parquet_size = os.path.getsize(parquet_path)\n",
    "ratio = csv_size / parquet_size\n",
    "\n",
    "print(f\"CSV size:     {csv_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Parquet size: {parquet_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Ratio:        Parquet is {ratio:.1f}x smaller than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12674f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Read latency: full table\n",
    "\n",
    "> **Spec:** [Motivation](https://parquet.apache.org/docs/overview/motivation/)\n",
    "\n",
    "Even reading the entire file, Parquet is faster because the on-disk data is\n",
    "more compact (less I/O) and each column is decoded with type-specific routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92507c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read CSV: 189.0 ms, 2713.9 CPU ms, (avg of 5 runs)\n",
      "Read Parquet: 118.7 ms, 1169.1 CPU ms, (avg of 5 runs)\n",
      "Speedup: Parquet is  1.6x faster to read than CSV\n"
     ]
    }
   ],
   "source": [
    "RUNS = 3\n",
    "\n",
    "def bench(fn, label):\n",
    "    times = []\n",
    "    cpu_times = []\n",
    "    for _ in range(RUNS):\n",
    "        t0 = time.perf_counter()\n",
    "        cpu_t0 = time.process_time()\n",
    "        fn()\n",
    "        times.append(time.perf_counter() - t0)\n",
    "        cpu_times.append(time.process_time() - cpu_t0)\n",
    "    avg = sum(times) / RUNS\n",
    "    cpu_avg = sum(cpu_times) / RUNS\n",
    "    print(f\"{label}: {avg * 1000:.1f} ms, {cpu_avg * 1000:.1f} CPU ms, (avg of {RUNS} runs)\")\n",
    "    return avg\n",
    "\n",
    "t_csv = bench(lambda: pa_csv.read_csv(csv_path), \"Read CSV\")\n",
    "t_pq  = bench(lambda: pq.read_table(parquet_path), \"Read Parquet\")\n",
    "print(f\"Speedup: Parquet is  {t_csv / t_pq:.1f}x faster to read than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799776bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Column projection: read only what you need\n",
    "\n",
    "> **Spec:** [Concepts: Unit of parallelization](https://parquet.apache.org/docs/concepts/)\n",
    "\n",
    "Parquet stores each column chunk contiguously on disk.  \n",
    "Requesting a subset of columns means the other column chunks are **never read from disk**.\n",
    "\n",
    "CSV has no such capability, every byte must be scanned to find field boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "590b15d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV   (all columns, no choice): 174.0 ms, 2500.7 CPU ms, (avg of 5 runs)\n",
      "Parquet (1 column projected)   : 61.3 ms, 304.2 CPU ms, (avg of 5 runs)\n",
      "Speedup from projection: 2.8x\n"
     ]
    }
   ],
   "source": [
    "t_csv_proj = bench(\n",
    "    lambda: pa_csv.read_csv(csv_path),  # CSV must still read everything\n",
    "    \"CSV   (all columns, no choice)\"\n",
    ")\n",
    "\n",
    "t_pq_proj = bench(\n",
    "    lambda: pq.read_table(parquet_path, columns=[\"value\"]),  # Only 1 of 4 columns\n",
    "    \"Parquet (1 column projected)   \"\n",
    ")\n",
    "\n",
    "print(f\"Speedup from projection: {t_csv_proj / t_pq_proj:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6eed44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key point |\n",
    "|---------|----------|\n",
    "| Columnar storage | Values of the same column are stored together, it's ideal for aggregations |\n",
    "| Compression | Type-aware encoding (dictionary, delta, RLE) dramatically reduces size |\n",
    "| Column projection | Only columns actually requested are read from disk |\n",
    "| Row group size | Smaller groups -> finer predicate skipping. Larger groups -> better compression & throughput |\n",
    "| Ecosystem | `pyarrow.parquet` wraps the Arrow C++ Parquet library|\n",
    "\n",
    "**Next ⏭️** [File format](02_file_format.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee04e0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Extra: Row group size, the tuning knob for predicate pushdown\n",
    "\n",
    "> **Spec:** [Concepts](https://parquet.apache.org/docs/concepts/)\n",
    "\n",
    "Each row group stores its own min/max statistics per column.\n",
    "When a filter is applied, the reader evaluates those statistics and skips entire\n",
    "row groups that cannot contain matching rows.\n",
    "\n",
    "**The trade-off:**\n",
    "- **Fewer, larger row groups** → better compression and sequential I/O throughput, but coarser skipping.\n",
    "- **More, smaller row groups** → better parallelization, finer-grained predicate skipping, but higher metadata overhead and more random I/O.\n",
    "\n",
    "We write the same table four times with different `row_group_size` values and then\n",
    "benchmark a highly selective filter to observe the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rg_10k: row_group_size=   10,000  -> 2000 row groups    file size: 383.1 MB\n",
      "  rg_100k: row_group_size=  100,000  ->  200 row groups    file size: 392.4 MB\n",
      "  rg_500k: row_group_size=  500,000  ->   40 row groups    file size: 335.7 MB\n",
      "  rg_2M: row_group_size=2,000,000  ->   10 row groups    file size: 320.2 MB\n",
      "\n",
      "Filter: id >= 19,800,000  (200,000 matching rows out of 20,000,000)\n",
      "\n",
      "Config          RG size  # RGs   Read time (ms)  RGs skipped\n",
      "--------------------------------------------------------------\n",
      "rg_10k           10,000   2000           33.6 ms    1980/2000 skipped\n",
      "rg_100k         100,000    200           14.5 ms     198/200 skipped\n",
      "rg_500k         500,000     40           18.1 ms      39/40 skipped\n",
      "rg_2M         2,000,000     10           56.1 ms       9/10 skipped\n",
      "\n",
      "Best config: rg_100k (14.5 ms:  3.9x faster than rg_2M (56.1 ms)\n"
     ]
    }
   ],
   "source": [
    "# Write the same table with four different row group sizes\n",
    "rg_configs = {\n",
    "    \"rg_10k\":  10_000,\n",
    "    \"rg_100k\": 100_000,\n",
    "    \"rg_500k\": 500_000,\n",
    "    \"rg_2M\":   2_000_000,\n",
    "}\n",
    "\n",
    "paths_rg = {}\n",
    "for name, rg_size in rg_configs.items():\n",
    "    p = f\"/tmp/sample_{name}.parquet\"\n",
    "    pq.write_table(table, p, compression=\"zstd\", row_group_size=rg_size)\n",
    "    meta = pq.read_metadata(p)\n",
    "    paths_rg[name] = (p, rg_size, meta.num_row_groups)\n",
    "    print(f\"  {name}: row_group_size={rg_size:>9,}  -> {meta.num_row_groups:>4} row groups  \"\n",
    "          f\"  file size: {os.path.getsize(p) / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Benchmark a selective filter: only the last 1 % of rows match\n",
    "threshold = int(N * 0.99)\n",
    "filter_expr = [(\"id\", \">=\", threshold)]\n",
    "\n",
    "print(f\"Filter: id >= {threshold:,}  ({N - threshold:,} matching rows out of {N:,})\\n\")\n",
    "print(f\"{'Config':<12} {'RG size':>10} {'# RGs':>6} {'Read time (ms)':>16} {'RGs skipped':>12}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "results_rg = {}\n",
    "for name, (p, rg_size, num_rgs) in paths_rg.items():\n",
    "    times = []\n",
    "    for _ in range(RUNS):\n",
    "        t0 = time.perf_counter()\n",
    "        pq.read_table(p, filters=filter_expr)\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    avg_ms = sum(times) / RUNS * 1000\n",
    "\n",
    "    # Count how many RGs are actually skipped by inspecting statistics\n",
    "    pf = pq.ParquetFile(p)\n",
    "    meta = pf.metadata\n",
    "    skipped = 0\n",
    "    for rg_idx in range(meta.num_row_groups):\n",
    "        rg = meta.row_group(rg_idx)\n",
    "        for col_idx in range(meta.num_columns):\n",
    "            col = rg.column(col_idx)\n",
    "            if col.path_in_schema == \"id\":\n",
    "                if col.statistics.max < threshold:\n",
    "                    skipped += 1\n",
    "                break\n",
    "\n",
    "    results_rg[name] = avg_ms\n",
    "    print(f\"{name:<12} {rg_size:>10,} {num_rgs:>6}   {avg_ms:>12.1f} ms   {skipped:>5}/{num_rgs} skipped ({skipped / num_rgs * 100:.1f}%)\")\n",
    "\n",
    "best = min(results_rg, key=results_rg.get)\n",
    "worst = max(results_rg, key=results_rg.get)\n",
    "print(f\"\\nBest config: {best} ({results_rg[best]:.1f} ms:  \"\n",
    "      f\"{results_rg[worst] / results_rg[best]:.1f}x faster than {worst} ({results_rg[worst]:.1f} ms)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

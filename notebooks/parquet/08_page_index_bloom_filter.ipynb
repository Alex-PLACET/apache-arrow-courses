{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9dc4f3",
   "metadata": {},
   "source": [
    "# Page Index & Bloom Filters\n",
    "\n",
    "> **Level:** Advanced  \n",
    "> **Spec:** [Page Index](https://parquet.apache.org/docs/file-format/pageindex/) · [Bloom Filter](https://parquet.apache.org/docs/file-format/bloomfilter/)  \n",
    "> **PyArrow docs:** [Parquet Datasets](https://arrow.apache.org/docs/python/parquet.html#partitioned-datasets)\n",
    "\n",
    "**What you will learn:**\n",
    "\n",
    "1. How column statistics enable row-group skipping for range predicates\n",
    "2. How the page index extends statistics to the page level for finer-grained skipping\n",
    "3. How bloom filters accelerate point-lookup queries on high-cardinality columns\n",
    "4. How to write files with bloom filters enabled using PyArrow\n",
    "5. How to verify skipping behaviour by reading individual row groups manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2378a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428e106",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Row-group skipping with column statistics\n",
    "\n",
    "> **Spec:** [Metadata: statistics](https://parquet.apache.org/docs/file-format/metadata/)\n",
    "\n",
    "Each column chunk stores `min` and `max` statistics in the file metadata.\n",
    "A reader evaluating a filter like `ts >= threshold` can skip any row group where `max(ts) < threshold`, without reading a single data byte.\n",
    "\n",
    "This is the most fundamental form of predicate pushdown in Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d01984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row groups: 3\n",
      "\n",
      " RG     ts_min     ts_max\n",
      "---------------------------\n",
      "  0          0      99999\n",
      "  1     100000     199999\n",
      "  2     200000     299999\n"
     ]
    }
   ],
   "source": [
    "# Build a table sorted by 'ts' so that each row group covers a distinct range\n",
    "N = 300_000\n",
    "RG_SIZE = 100_000  # 3 row groups\n",
    "\n",
    "table = pa.table({\n",
    "    \"ts\":     pa.array(range(N), type=pa.int64()),        # monotone ➡️ min/max are tight\n",
    "    \"value\":  pa.array([float(i) * 0.01 for i in range(N)]),\n",
    "    \"label\":  pa.array([\"a\", \"b\", \"c\"][i % 3] for i in range(N)),\n",
    "})\n",
    "\n",
    "path = \"/tmp/predpush.parquet\"\n",
    "pq.write_table(table, path, row_group_size=RG_SIZE, write_statistics=True)\n",
    "\n",
    "pf = pq.ParquetFile(path)\n",
    "meta = pf.metadata\n",
    "\n",
    "print(f\"Row groups: {meta.num_row_groups}\")\n",
    "print()\n",
    "print(f\"{'RG':>3} {'ts_min':>10} {'ts_max':>10}\")\n",
    "print(\"-\" * 27)\n",
    "for rg_idx in range(meta.num_row_groups):\n",
    "    rg = meta.row_group(rg_idx)\n",
    "    for col_idx in range(meta.num_columns):\n",
    "        col = rg.column(col_idx)\n",
    "        if col.path_in_schema == \"ts\":\n",
    "            stats = col.statistics\n",
    "            print(f\"{rg_idx:>3} {stats.min:>10} {stats.max:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e1b5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Applying a filter: PyArrow's predicate pushdown\n",
    "\n",
    "> **Spec:** [Page Index](https://parquet.apache.org/docs/file-format/pageindex/)\n",
    "\n",
    "`pq.read_table(filters=...)` passes the predicate to the C++ Parquet reader,\n",
    "which applies row-group skipping based on statistics before decoding any pages.\n",
    "We can observe this by timing filtered vs full reads and by reading row groups manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b30563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full table read (no filter)  : 11.6 ms, 300,000 rows returned\n",
      "Filtered read (ts >= 200000): 3.8 ms, 100,000 rows returned\n",
      "\n",
      "Speedup from row-group skipping: 3.1x\n"
     ]
    }
   ],
   "source": [
    "RUNS = 3\n",
    "\n",
    "def bench(fn, label):\n",
    "    times = []\n",
    "    for _ in range(RUNS):\n",
    "        t0 = time.perf_counter()\n",
    "        result = fn()\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    avg = sum(times) / RUNS\n",
    "    print(f\"{label}: {avg * 1000:.1f} ms, {result.num_rows:,} rows returned\")\n",
    "    return avg\n",
    "\n",
    "# Read all rows\n",
    "t_full = bench(\n",
    "    lambda: pq.read_table(path),\n",
    "    \"Full table read (no filter)  \"\n",
    ")\n",
    "\n",
    "# Read only rows where ts >= 200_000, should skip the first 2 row groups\n",
    "t_filtered = bench(\n",
    "    lambda: pq.read_table(path, filters=[(\"ts\", \">=\", 200_000)]),\n",
    "    \"Filtered read (ts >= 200000)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nSpeedup from row-group skipping: {t_full / t_filtered:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab31d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Manual row-group inspection to confirm skipping\n",
    "\n",
    "> **Spec:** [Metadata: Row group statistics](https://parquet.apache.org/docs/file-format/metadata/)\n",
    "\n",
    "We can replicate the reader's logic manually: walk row groups, check statistics,\n",
    "and only call `read_row_group(i)` for groups that pass the predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8a965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual predicate pushdown for ts >= 200000:\n",
      "\n",
      "  Row group 0: max=99999 ⏭️ SKIP\n",
      "  Row group 1: max=199999 ⏭️ SKIP\n",
      "  Row group 2: max=299999 ⏭️ READ\n",
      "\n",
      "Result rows: 100,000\n",
      "Expected:    100,000\n"
     ]
    }
   ],
   "source": [
    "filter_value = 200_000\n",
    "pf2 = pq.ParquetFile(path)\n",
    "meta2 = pf2.metadata\n",
    "\n",
    "print(f\"Manual predicate pushdown for ts >= {filter_value}:\\n\")\n",
    "\n",
    "batches = []\n",
    "for rg_idx in range(meta2.num_row_groups):\n",
    "    rg = meta2.row_group(rg_idx)\n",
    "    for col_idx in range(meta2.num_columns):\n",
    "        col = rg.column(col_idx)\n",
    "        if col.path_in_schema == \"ts\":\n",
    "            stats = col.statistics\n",
    "            if stats.max >= filter_value:\n",
    "                print(f\"  Row group {rg_idx}: max={stats.max} ⏭️ READ\")\n",
    "                batches.append(pf2.read_row_group(rg_idx))\n",
    "            else:\n",
    "                print(f\"  Row group {rg_idx}: max={stats.max} ⏭️ SKIP\")\n",
    "\n",
    "result_manual = pa.concat_tables(batches)\n",
    "# Apply remaining row-level filter in memory\n",
    "mask = pa.compute.greater_equal(result_manual.column(\"ts\"), filter_value)\n",
    "result_manual = result_manual.filter(mask)\n",
    "\n",
    "print(f\"\\nResult rows: {result_manual.num_rows:,}\")\n",
    "print(f\"Expected:    {N - filter_value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9769a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bloom filters: point-lookup acceleration\n",
    "\n",
    "> **Spec:** [Bloom Filter](https://parquet.apache.org/docs/file-format/bloomfilter/)\n",
    "\n",
    "Column statistics (min/max) work well for range predicates on sorted data.\n",
    "For **equality predicates on high-cardinality columns** (e.g., `user_id = 'abc123'`),\n",
    "a bloom filter provides probabilistic membership testing:\n",
    "- A **definite NO** allows the row group to be skipped entirely\n",
    "- A **possible YES** means the row group must be read (false positives are possible)\n",
    "\n",
    "> **PyArrow 23 note:** The Python bindings do not yet expose a `WriterProperties`\n",
    "> class or a `bloom_filter_enabled` keyword for writing bloom filters. The Parquet\n",
    "> C++ library supports them, but the API surface has not been surfaced in Python.\n",
    "> Equality-predicate pushdown still works through **row-group min/max statistics**,\n",
    "> just without the probabilistic bloom filter layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d805299",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Row-group statistics confirm skipping for the equality predicate\n",
    "\n",
    "> **Spec:** [Bloom Filter: BloomFilterHeader](https://parquet.apache.org/docs/file-format/bloomfilter/)\n",
    "\n",
    "When bloom filters **are** written, Parquet stores a `bloom_filter_offset` byte\n",
    "offset in `ColumnChunkMetaData`. A non-zero offset means a bloom filter is present.\n",
    "\n",
    "Because PyArrow 23 does not write bloom filters from Python, all offsets below\n",
    "will be `None`. The cell also shows min/max statistics, confirming that the\n",
    "sorted `user_id` column still enables row-group skipping via statistics alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ba1e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Technique | Predicate type | Mechanism | PyArrow 23 support |\n",
    "|-----------|---------------|----------|--------------------|\n",
    "| Column statistics (min/max) | Range: `col >= x`, `col BETWEEN a AND b` | Stored in FileMetaData footer, zero seeks to evaluate | ✅ read & write |\n",
    "| Page index | Range at page granularity | Finer than row-group stats, stored after footer | ✅ write (`write_page_index=True`), ⚠️ not yet used on read |\n",
    "| Bloom filter | Equality: `col == value` | Probabilistic set membership. definite-no ➡️ skip row group | ⚠️ C++ only; Python write API not yet exposed |\n",
    "\n",
    "All three techniques skip data **before any column chunk data is decompressed or decoded**, they operate purely on metadata.\n",
    "\n",
    "**End of the Parquet series.** Return to the index: [Intro](00_intro.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c50f1f",
   "metadata": {},
   "source": [
    "# Compression\n",
    "\n",
    "> **Level:** Intermediate  \n",
    "> **Spec:** [Compression](https://parquet.apache.org/docs/file-format/data-pages/compression/)  \n",
    "> **PyArrow docs:** [write_table - compression](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html)\n",
    "\n",
    "**What you will learn:**\n",
    "\n",
    "1. The compression codecs supported by Parquet: SNAPPY, GZIP, ZSTD, LZ4_RAW, NONE\n",
    "2. How to compare codec performance (size vs read latency) on the same data\n",
    "3. How to set per-column compression overrides with `ParquetWriter`\n",
    "4. When to choose which codec for analytical vs archival workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d3f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7dc7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Supported codecs\n",
    "\n",
    "> **Spec:** [Compression codecs](https://parquet.apache.org/docs/file-format/data-pages/compression/)\n",
    "\n",
    "| Codec | Key characteristic |\n",
    "|-------|-------------------|\n",
    "| `UNCOMPRESSED` | No-op: fastest, largest |\n",
    "| `SNAPPY` | Google Snappy: fast with moderate ratio |\n",
    "| `GZIP` | RFC 1952: high ratio, slower |\n",
    "| `ZSTD` | Zstandard: tunable ratio/speed, best modern default |\n",
    "| `LZ4_RAW` | LZ4 block format: very fast, low ratio |\n",
    "| `BROTLI` | RFC 7932: high ratio, slow (web-oriented) |\n",
    "| `LZ4` | **Deprecated**: non-standard framing, avoid in new files |\n",
    "\n",
    "Compression acts on the **page-level data** after encoding. A SNAPPY-compressed ZSTD page\n",
    "is not possible: one codec per column chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6741d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 200,000\n",
      "Columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Build a mixed-type table suitable for compression benchmarking\n",
    "N = 200_000\n",
    "categories = [\"north\", \"south\", \"east\", \"west\"]\n",
    "\n",
    "table = pa.table({\n",
    "    \"id\":       pa.array(range(N), type=pa.int64()),\n",
    "    \"value\":    pa.array([float(i) * 0.123 for i in range(N)], type=pa.float64()),\n",
    "    \"region\":   pa.array([categories[i % 4] for i in range(N)], type=pa.string()),\n",
    "    \"note\":     pa.array([f\"record-{i:07d}\" for i in range(N)], type=pa.string()),\n",
    "})\n",
    "\n",
    "print(f\"Rows: {table.num_rows:,}\")\n",
    "print(f\"Columns: {table.num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce856316",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Codec comparison: size and read speed\n",
    "\n",
    "> **Spec:** [Compression: Overview](https://parquet.apache.org/docs/file-format/data-pages/compression/)\n",
    "\n",
    "We write identical data with each codec and measure:\n",
    "- Final file size (on-disk bytes)\n",
    "- Read latency (`time.perf_counter()` average over 3 runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6691f1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower is better. None is the baseline (1.00x).\n",
      "Codec         Size (MB)     Ratio  Read (ms)  Read Ratio\n",
      "--------------------------------------------------------\n",
      "NONE               7.16     1.00x        6.5       1.00x\n",
      "SNAPPY             3.32     0.46x        6.6       1.02x\n",
      "GZIP               1.97     0.28x        9.8       1.50x\n",
      "ZSTD               1.26     0.18x        5.6       0.86x\n",
      "LZ4                3.19     0.45x        5.0       0.77x\n",
      "BROTLI             1.22     0.17x        7.7       1.18x\n"
     ]
    }
   ],
   "source": [
    "CODECS = [\"NONE\", \"SNAPPY\", \"GZIP\", \"ZSTD\", \"LZ4\", \"BROTLI\"]\n",
    "RUNS = 3\n",
    "results = []\n",
    "\n",
    "for codec in CODECS:\n",
    "    path = f\"/tmp/sample_{codec.lower()}.parquet\"\n",
    "\n",
    "    # Write\n",
    "    pq.write_table(table, path, compression=codec)\n",
    "    size = os.path.getsize(path)\n",
    "\n",
    "    # Read (average of RUNS)\n",
    "    times = []\n",
    "    for _ in range(RUNS):\n",
    "        t0 = time.perf_counter()\n",
    "        pq.read_table(path)\n",
    "        times.append(time.perf_counter() - t0)\n",
    "    avg_ms = sum(times) / RUNS * 1000\n",
    "\n",
    "    results.append((codec, size, avg_ms))\n",
    "\n",
    "# Baseline: NONE\n",
    "base_size = results[0][1]\n",
    "print(\"Lower is better. None is the baseline (1.00x).\")\n",
    "print(f\"{'Codec':<12} {'Size (MB)':>10} {'Ratio':>9} {'Read (ms)':>10} {'Read Ratio':>11}\")\n",
    "print(\"-\" * 56)\n",
    "for codec, size, avg_ms in results:\n",
    "    ratio = size / base_size\n",
    "    read_ratio = avg_ms / results[0][2]\n",
    "    print(f\"{codec:<12} {size/1024/1024:>10.2f} {ratio:>8.2f}x {avg_ms:>10.1f} {read_ratio:>10.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821cccf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Per-column compression overrides\n",
    "\n",
    "> **Spec:** [Compression: per column](https://parquet.apache.org/docs/file-format/data-pages/compression/)\n",
    "\n",
    "Parquet stores compression per column chunk. You can assign different codecs to\n",
    "different columns, for example, use ZSTD for compressible text columns and\n",
    "LZ4_RAW for numeric columns where speed matters more than ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecbea253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-column compression in the written file:\n",
      "  id         ➡️ LZ4  (1,072,935 bytes)\n",
      "  value      ➡️ LZ4  (1,312,007 bytes)\n",
      "  region     ➡️ ZSTD  (1,787 bytes)\n",
      "  note       ➡️ ZSTD  (189,961 bytes)\n"
     ]
    }
   ],
   "source": [
    "per_col_path = \"/tmp/sample_per_col.parquet\"\n",
    "\n",
    "# id + value ➡️ LZ4_RAW (speed-first numerics)\n",
    "# region + note ➡️ ZSTD   (ratio-first text)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    per_col_path,\n",
    "    compression={\n",
    "        \"id\":     \"LZ4\",\n",
    "        \"value\":  \"LZ4\",\n",
    "        \"region\": \"ZSTD\",\n",
    "        \"note\":   \"ZSTD\",\n",
    "    },\n",
    ")\n",
    "\n",
    "pf = pq.ParquetFile(per_col_path)\n",
    "rg = pf.metadata.row_group(0)\n",
    "\n",
    "print(\"Per-column compression in the written file:\")\n",
    "for col_idx in range(pf.metadata.num_columns):\n",
    "    col = rg.column(col_idx)\n",
    "    print(f\"  {col.path_in_schema:<10} ➡️ {col.compression}  ({col.total_compressed_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35933d46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ZSTD compression level\n",
    "\n",
    "> **Spec:** [ZSTD](https://parquet.apache.org/docs/file-format/data-pages/compression/)\n",
    "\n",
    "ZSTD supports a compression level (1–22). Higher levels trade CPU time for smaller files.\n",
    "Level 1 is already competitive with SNAPPY (level 3 is the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fcd4176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSTD level    Size (MB)   Write (ms)\n",
      "-------------------------------------\n",
      "level 1            1.26         47.2\n",
      "level 3            1.33         39.1\n",
      "level 9            1.25         95.5\n",
      "level 19           1.13       1321.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'ZSTD level':<12} {'Size (MB)':>10} {'Write (ms)':>12}\")\n",
    "print(\"-\" * 37)\n",
    "for level in [1, 3, 9, 19]:\n",
    "    path = f\"/tmp/sample_zstd_{level}.parquet\"\n",
    "    t0 = time.perf_counter()\n",
    "    pq.write_table(table, path, compression=\"ZSTD\", compression_level=level)\n",
    "    write_ms = (time.perf_counter() - t0) * 1000\n",
    "    size = os.path.getsize(path)\n",
    "    print(f\"level {level:<6} {size/1024/1024:>10.2f} {write_ms:>12.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72accc3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Codec | Use when |\n",
    "|-------|----------|\n",
    "| `UNCOMPRESSED` | Benchmarking / NVMe storage where CPU is the bottleneck |\n",
    "| `SNAPPY` | Low-latency, high-throughput pipelines (legacy default) |\n",
    "| `ZSTD` | General-purpose: best modern default. Tune level per workload |\n",
    "| `LZ4_RAW` | Maximum decompression speed (streaming, real-time ingest) |\n",
    "| `GZIP` | Archival / cold storage where size matters more than speed |\n",
    "| Per-column | Mix codecs when columns have different compressibility profiles |\n",
    "\n",
    "**Next ⏭️** [Nested encoding](07_nested_encoding.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

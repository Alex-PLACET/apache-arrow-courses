{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7a4c94",
   "metadata": {},
   "source": [
    "# File Format\n",
    "\n",
    "> **Level:** Beginner  \n",
    "> **Spec:** [File Format](https://parquet.apache.org/docs/file-format/)  \n",
    "> **PyArrow docs:** [pyarrow.parquet.ParquetFile](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html)\n",
    "\n",
    "**What you will learn:**\n",
    "\n",
    "1. The physical layout of a Parquet file: magic bytes, data, footer\n",
    "2. The hierarchy: File → Row Groups → Column Chunks → Pages\n",
    "3. Why file metadata is written at the end (footer-first design)\n",
    "4. How to navigate the hierarchy programmatically with `ParquetFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4de21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import struct\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0f367",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Magic bytes: `PAR1`\n",
    "\n",
    "> **Spec:** [File Format](https://parquet.apache.org/docs/file-format/)\n",
    "\n",
    "Every valid Parquet file starts **and** ends with the 4-byte magic number `PAR1` (`0x50 0x41 0x52 0x31`).\n",
    "\n",
    "Layout:\n",
    "```\n",
    "[ PAR1 ]  [ column data ... ]  [ File Metadata ]  [ 4-byte footer length ]  [ PAR1 ]\n",
    "```\n",
    "\n",
    "Readers locate the footer by seeking to the last 8 bytes of the file:\n",
    "- bytes [-4:] must be `PAR1`\n",
    "- bytes [-8:-4] hold the footer length as a little-endian `uint32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a4e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 457 bytes\n",
      "\n",
      "First 4 bytes: b'PAR1'  →  valid: True\n",
      "Last  4 bytes: b'PAR1'  →  valid: True\n",
      "Footer length (bytes [-8:-4]): 358 bytes\n",
      "Footer starts at byte offset:  91\n"
     ]
    }
   ],
   "source": [
    "# Write a minimal table to a BytesIO buffer so we can inspect the raw bytes\n",
    "table = pa.table({\"x\": pa.array([1, 2, 3], type=pa.int32())})\n",
    "buf = io.BytesIO()\n",
    "pq.write_table(table, buf)\n",
    "data = buf.getvalue()\n",
    "\n",
    "magic = b\"PAR1\"\n",
    "\n",
    "print(f\"File size: {len(data)} bytes\")\n",
    "print()\n",
    "\n",
    "# Leading magic\n",
    "leading = data[:4]\n",
    "print(f\"First 4 bytes: {leading!r}  ➡️  valid: {leading == magic}\")\n",
    "\n",
    "# Trailing magic\n",
    "trailing = data[-4:]\n",
    "print(f\"Last  4 bytes: {trailing!r}  ➡️  valid: {trailing == magic}\")\n",
    "\n",
    "# Footer length (little-endian uint32 at bytes [-8:-4])\n",
    "footer_len = struct.unpack_from(\"<I\", data, len(data) - 8)[0]\n",
    "print(f\"Footer length (bytes [-8:-4]): {footer_len} bytes\")\n",
    "print(f\"Footer starts at byte offset:  {len(data) - 8 - footer_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed02ba6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Hierarchy: Row Groups → Column Chunks → Pages\n",
    "\n",
    "> **Spec:** [Concepts](https://parquet.apache.org/docs/concepts/)\n",
    "\n",
    "```\n",
    "File\n",
    " └─ Row Group 0          (horizontal slice of rows)\n",
    "     ├─ Column Chunk 0   (all values of column 0 in this row group)\n",
    "     │   ├─ Page 0       (smallest unit: compressed + encoded)\n",
    "     │   └─ Page 1\n",
    "     └─ Column Chunk 1\n",
    "         └─ Page 0\n",
    " └─ Row Group 1\n",
    "     ...\n",
    "```\n",
    "\n",
    "- **Row Group**: logical horizontal partitioning. Parallelism unit for MapReduce / distributed engines.\n",
    "- **Column Chunk**: all values of one column within a row group. Contiguous on disk.\n",
    "- **Page**: indivisible unit for compression and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9cf594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:   50,000\n",
      "Row groups:   3\n",
      "Columns:      3\n",
      "\n",
      "  Row group 0: 20,000 rows, 237,850 bytes uncompressed\n",
      "    Column 'id': 117,636 bytes compressed\n",
      "    Column 'score': 117,637 bytes compressed\n",
      "    Column 'label': 195 bytes compressed\n",
      "  Row group 1: 20,000 rows, 237,850 bytes uncompressed\n",
      "    Column 'id': 117,637 bytes compressed\n",
      "    Column 'score': 117,637 bytes compressed\n",
      "    Column 'label': 195 bytes compressed\n",
      "  Row group 2: 10,000 rows, 116,540 bytes uncompressed\n",
      "    Column 'id': 57,614 bytes compressed\n",
      "    Column 'score': 57,614 bytes compressed\n",
      "    Column 'label': 135 bytes compressed\n"
     ]
    }
   ],
   "source": [
    "# Write a larger table split across multiple row groups\n",
    "N = 50_000\n",
    "big_table = pa.table({\n",
    "    \"id\":    pa.array(range(N), type=pa.int32()),\n",
    "    \"score\": pa.array([float(i) / N for i in range(N)], type=pa.float32()),\n",
    "    \"label\": pa.array([\"a\" if i % 2 == 0 else \"b\" for i in range(N)], type=pa.string()),\n",
    "})\n",
    "\n",
    "buf2 = io.BytesIO()\n",
    "# row_group_size controls the max number of rows per row group\n",
    "pq.write_table(big_table, buf2, row_group_size=20_000)\n",
    "buf2.seek(0)\n",
    "\n",
    "pf = pq.ParquetFile(buf2)\n",
    "meta = pf.metadata\n",
    "\n",
    "print(f\"Total rows:   {meta.num_rows:,}\")\n",
    "print(f\"Row groups:   {meta.num_row_groups}\")\n",
    "print(f\"Columns:      {meta.num_columns}\")\n",
    "print()\n",
    "\n",
    "for rg_idx in range(meta.num_row_groups):\n",
    "    rg = meta.row_group(rg_idx)\n",
    "    print(f\"  Row group {rg_idx}: {rg.num_rows:,} rows, {rg.total_byte_size:,} bytes uncompressed\")\n",
    "    for col_idx in range(meta.num_columns):\n",
    "        col = rg.column(col_idx)\n",
    "        print(f\"    Column '{col.path_in_schema}': {col.total_compressed_size:,} bytes compressed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5bd841",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Footer-first design\n",
    "\n",
    "> **Spec:** [File Format: metadata after data](https://parquet.apache.org/docs/file-format/)\n",
    "\n",
    "File metadata is written **after** all column data. This allows single-pass writing:\n",
    "a writer streams column chunks without needing to know final offsets upfront,\n",
    "then appends the complete metadata, including all column chunk offsets, at the end.\n",
    "\n",
    "A reader therefore:\n",
    "1. Seeks to EOF − 8 bytes to read the footer length\n",
    "2. Seeks back by that length to read the full `FileMetaData` Thrift struct\n",
    "3. Uses the column chunk offsets in the metadata to seek to each column chunk\n",
    "\n",
    "All column chunk offsets are stored in the file footer, random-access column reads require exactly **two seeks** before any data is read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba82ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: column chunk file offsets are stored in the metadata (not inline)\n",
    "buf2.seek(0)\n",
    "pf2 = pq.ParquetFile(buf2)\n",
    "meta2 = pf2.metadata\n",
    "\n",
    "file_size = len(buf2.getvalue())\n",
    "footer_length = struct.unpack_from(\"<I\", buf2.getvalue(), file_size - 8)[0]\n",
    "data_region_end = file_size - 8 - footer_length\n",
    "\n",
    "print(f\"File size:               {file_size:,} bytes\")\n",
    "print(f\"Footer length:           {footer_length:,} bytes\")\n",
    "print(f\"Data region: bytes 4 .. {data_region_end:,}\")\n",
    "print()\n",
    "print(\"Column chunk offsets (from footer metadata):\")\n",
    "for rg_idx in range(meta2.num_row_groups):\n",
    "    rg = meta2.row_group(rg_idx)\n",
    "    for col_idx in range(meta2.num_columns):\n",
    "        col = rg.column(col_idx)\n",
    "        print(f\"  RG {rg_idx} / col '{col.path_in_schema}': offset = {col.file_offset:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fe625",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Reading individual row groups\n",
    "\n",
    "> **Spec:** [Concepts: Row group](https://parquet.apache.org/docs/concepts/)\n",
    "\n",
    "`ParquetFile.read_row_group(i)` reads exactly one row group without touching the others.\n",
    "This is the basis for parallel and incremental processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934747a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf2.seek(0)\n",
    "pf3 = pq.ParquetFile(buf2)\n",
    "\n",
    "for rg_idx in range(pf3.metadata.num_row_groups):\n",
    "    batch = pf3.read_row_group(rg_idx)\n",
    "    ids = batch.column(\"id\")\n",
    "    print(f\"Row group {rg_idx}: rows {ids[0].as_py()} .. {ids[-1].as_py()} ({len(ids):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9d03f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key point |\n",
    "|---------|----------|\n",
    "| Magic bytes | Files start and end with `PAR1` for quick validity check |\n",
    "| Footer-first | Metadata written after data for single-pass writing, two-seek reading |\n",
    "| Row group | Horizontal partition: controls parallelism and memory during read |\n",
    "| Column chunk | Contiguous on disk per-column per-row-group which enables column projection |\n",
    "| Page | Smallest compression/encoding unit inside a column chunk |\n",
    "\n",
    "**Next ⏭️** [Metadata](03_metadata.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

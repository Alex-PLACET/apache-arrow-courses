{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004b6317",
   "metadata": {},
   "source": [
    "# Encodings\n",
    "\n",
    "> **Level:** Intermediate  \n",
    "> **Spec:** [Encodings](https://parquet.apache.org/docs/file-format/data-pages/encodings/)  \n",
    "> **PyArrow docs:** [ParquetWriter](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetWriter.html)\n",
    "\n",
    "**What you will learn:**\n",
    "\n",
    "1. How `PLAIN` encoding stores values back-to-back with no compression\n",
    "2. How `RLE_DICTIONARY` slashes size for low-cardinality columns\n",
    "3. How `DELTA_BINARY_PACKED` exploits monotone integer sequences\n",
    "4. How `BYTE_STREAM_SPLIT` improves floating-point compressibility\n",
    "5. How to read back the encodings actually used by PyArrow from column-chunk metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef38158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419312c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PLAIN encoding: simple back-to-back storage\n",
    "\n",
    "> **Spec:** [Encodings: PLAIN](https://parquet.apache.org/docs/file-format/data-pages/encodings/)\n",
    "\n",
    "PLAIN is the baseline encoding. Values are stored consecutively with no further transformation:\n",
    "- `INT32`: 4 bytes little-endian per value\n",
    "- `BYTE_ARRAY`: `[4-byte length][bytes]` per value\n",
    "\n",
    "PLAIN must be supported for all types. High-cardinality or random columns fall back to PLAIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: uuid_like\n",
      "Encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n",
      "Compressed:   598,337 bytes\n",
      "Uncompressed: 697,794 bytes\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "rng = random.Random(42)\n",
    "N = 50_000\n",
    "\n",
    "# High-cardinality string column, essentially random, dictionary will be large -> falls back to PLAIN\n",
    "random_strings = [\"\".join(rng.choices(string.ascii_letters, k=8)) for _ in range(N)]\n",
    "table_plain = pa.table({\"uuid_like\": pa.array(random_strings, type=pa.string())})\n",
    "\n",
    "buf_plain = io.BytesIO()\n",
    "pq.write_table(table_plain, buf_plain)\n",
    "buf_plain.seek(0)\n",
    "\n",
    "meta = pq.ParquetFile(buf_plain).metadata\n",
    "col = meta.row_group(0).column(0)\n",
    "print(f\"Column: {col.path_in_schema}\")\n",
    "print(f\"Encodings: {col.encodings}\")\n",
    "print(f\"Compressed:   {col.total_compressed_size:,} bytes\")\n",
    "print(f\"Uncompressed: {col.total_uncompressed_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51777b97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. RLE_DICTIONARY encoding: low-cardinality columns\n",
    "\n",
    "> **Spec:** [Encodings: Dictionary Encoding](https://parquet.apache.org/docs/file-format/data-pages/encodings/)\n",
    "\n",
    "Dictionary encoding builds a per-column-chunk lookup table of distinct values.\n",
    "Data pages store integer indices into that table, encoded with RLE / bit-packing.\n",
    "\n",
    "For a column with only 5 distinct values repeated 50 000 times:\n",
    "- Dictionary page: 5 entries\n",
    "- Data pages: RLE-encoded integers in the range [0..4], 3 bits each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df5a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: category\n",
      "Encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n",
      "Compressed:   1,226 bytes\n",
      "Uncompressed: 19,060 bytes\n",
      "\n",
      "Dictionary encoding: 1,226 bytes\n",
      "Plain encoding:      22,074 bytes\n",
      "Dictionary is 18.0x smaller\n"
     ]
    }
   ],
   "source": [
    "categories = [\"alpha\", \"beta\", \"gamma\", \"delta\", \"epsilon\"]\n",
    "table_dict = pa.table({\"category\": pa.array([categories[i % 5] for i in range(N)], type=pa.string())})\n",
    "\n",
    "buf_dict = io.BytesIO()\n",
    "pq.write_table(table_dict, buf_dict, use_dictionary=True)\n",
    "buf_dict.seek(0)\n",
    "\n",
    "meta_dict = pq.ParquetFile(buf_dict).metadata\n",
    "col_dict = meta_dict.row_group(0).column(0)\n",
    "print(f\"Column: {col_dict.path_in_schema}\")\n",
    "print(f\"Encodings: {col_dict.encodings}\")\n",
    "print(f\"Compressed:   {col_dict.total_compressed_size:,} bytes\")\n",
    "print(f\"Uncompressed: {col_dict.total_uncompressed_size:,} bytes\")\n",
    "\n",
    "# Compare: plain encoding of the same data\n",
    "buf_no_dict = io.BytesIO()\n",
    "pq.write_table(table_dict, buf_no_dict, use_dictionary=False)\n",
    "size_plain = pq.ParquetFile(buf_no_dict).metadata.row_group(0).column(0).total_compressed_size\n",
    "print()\n",
    "print(f\"Dictionary encoding: {col_dict.total_compressed_size:,} bytes\")\n",
    "print(f\"Plain encoding:      {size_plain:,} bytes\")\n",
    "print(f\"Dictionary is {size_plain / col_dict.total_compressed_size:.1f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784d3d45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. DELTA_BINARY_PACKED: monotone integer sequences\n",
    "\n",
    "> **Spec:** [Encodings: Delta Encoding](https://parquet.apache.org/docs/file-format/data-pages/encodings/)\n",
    "\n",
    "Delta encoding stores the **differences** between consecutive values rather than absolute values.\n",
    "For a monotone sequence like `0, 1, 2, 3, …` every delta is 1, all deltas compute to 0 after\n",
    "subtracting the minimum delta, so they can be stored with **0 bits per value**.\n",
    "\n",
    "Delta encoding is enabled automatically for `INT32`/`INT64` columns when PyArrow detects\n",
    "it will produce smaller output than plain/dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8131b5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings: ('RLE', 'PLAIN')\n",
      "Compressed:   201,107 bytes\n",
      "Uncompressed: 400,231 bytes\n",
      "Theoretical minimum (PLAIN): 400,000 bytes  (8 bytes x 50,000 int64 values)\n"
     ]
    }
   ],
   "source": [
    "table_delta = pa.table({\"monotone_id\": pa.array(range(N), type=pa.int64())})\n",
    "\n",
    "# use_dictionary=False forces PyArrow to choose the best non-dict encoding (delta for int)\n",
    "buf_delta = io.BytesIO()\n",
    "pq.write_table(table_delta, buf_delta, use_dictionary=False)\n",
    "buf_delta.seek(0)\n",
    "\n",
    "meta_delta = pq.ParquetFile(buf_delta).metadata\n",
    "col_delta = meta_delta.row_group(0).column(0)\n",
    "print(f\"Encodings: {col_delta.encodings}\")\n",
    "print(f\"Compressed:   {col_delta.total_compressed_size:,} bytes\")\n",
    "print(f\"Uncompressed: {col_delta.total_uncompressed_size:,} bytes\")\n",
    "print(f\"Theoretical minimum (PLAIN): {N * 8:,} bytes  (8 bytes x {N:,} int64 values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9420e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. BYTE_STREAM_SPLIT: better floating-point compression\n",
    "\n",
    "> **Spec:** [Encodings: Byte Stream Split](https://parquet.apache.org/docs/file-format/data-pages/encodings/)\n",
    "\n",
    "BYTE_STREAM_SPLIT **transposes** the bytes of each floating-point value.\n",
    "For N `FLOAT` values, instead of `[B0 B1 B2 B3][B0 B1 B2 B3]…`, it emits:\n",
    "`[all B0s][all B1s][all B2s][all B3s]`. The total size is unchanged,\n",
    "but byte streams within each position are highly compressible by entropy coders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41f7178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAIN + snappy:             200,207 bytes\n",
      "BYTE_STREAM_SPLIT + snappy: 166,074 bytes\n",
      "Ratio: 1.21x smaller with BYTE_STREAM_SPLIT\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Smooth float sequence: exponent byte barely changes ➡️ B3 stream is near-constant\n",
    "floats = [math.sin(i * 0.001) for i in range(N)]\n",
    "table_float = pa.table({\"signal\": pa.array(floats, type=pa.float32())})\n",
    "\n",
    "# Default (no byte stream split, snappy compression)\n",
    "buf_plain_float = io.BytesIO()\n",
    "pq.write_table(table_float, buf_plain_float, use_dictionary=False, compression=\"snappy\")\n",
    "\n",
    "# With byte stream split + snappy\n",
    "buf_bss = io.BytesIO()\n",
    "pq.write_table(table_float, buf_bss,\n",
    "               use_dictionary=False,\n",
    "               use_byte_stream_split=True,\n",
    "               compression=\"snappy\")\n",
    "\n",
    "size_plain = pq.ParquetFile(buf_plain_float).metadata.row_group(0).column(0).total_compressed_size\n",
    "size_bss   = pq.ParquetFile(buf_bss).metadata.row_group(0).column(0).total_compressed_size\n",
    "\n",
    "print(f\"PLAIN + snappy:             {size_plain:,} bytes\")\n",
    "print(f\"BYTE_STREAM_SPLIT + snappy: {size_bss:,} bytes\")\n",
    "print(f\"Ratio: {size_plain / size_bss:.2f}x smaller with BYTE_STREAM_SPLIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d993f08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Encoding | Physical type | Best for |\n",
    "|----------|--------------|----------|\n",
    "| `PLAIN` | All | High-cardinality / random data |\n",
    "| `RLE_DICTIONARY` | All | Low-cardinality columns (categories, booleans, enums) |\n",
    "| `DELTA_BINARY_PACKED` | INT32, INT64 | Monotone or slowly-changing integer sequences |\n",
    "| `DELTA_LENGTH_BYTE_ARRAY` | BYTE_ARRAY | Variable-length strings of similar lengths |\n",
    "| `DELTA_BYTE_ARRAY` | BYTE_ARRAY | Sorted strings with shared prefixes |\n",
    "| `BYTE_STREAM_SPLIT` | FLOAT, DOUBLE | Smooth floating-point data (before applying a compressor) |\n",
    "\n",
    "**Next ⏭️** [Compression](06_compression.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
